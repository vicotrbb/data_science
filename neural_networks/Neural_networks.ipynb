{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicotrbb/machine_learning/blob/master/neural_networks/Neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNIBHFAQ9LCM",
        "colab_type": "text"
      },
      "source": [
        "# Neural networks\n",
        "\n",
        "\n",
        "### Referencias\n",
        "\n",
        "* https://matheusfacure.github.io/2017/07/12/activ-func/#sig\n",
        "* https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
        "* https://en.wikipedia.org/wiki/Sigmoid_function#Applications\n",
        "* https://nextjournal.com/gkoehler/machine-translation-seq2seq-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC_MAzUSxauL",
        "colab_type": "text"
      },
      "source": [
        "## Oque é uma rede neural\n",
        "\n",
        "Redes neurais são um sistema de computação inspiradas por redes neurais biologicas que formam o cerebro de seres vivos. Estes sistemas são capazes de \"aprender\" a efetuar tarefas, geralmente sem a necessidade de programar as rotinas das tarefas previamente.\n",
        "\n",
        "Um rede neural é composta por 3 tipos de camadas:\n",
        "\n",
        "* **Camada de entrada** -> A camada de entrada é responsavel por realizar o input de informações;\n",
        "\n",
        "* **Camada oculta** -> É a camada intermediaria entre a de entrada e de saída e é responsavel por performar toda a computação(pode haver mais de uma);\n",
        "\n",
        "* **Camada de saída** -> A camada de saída é responsavel por realizar o output da informação.\n",
        "\n",
        "<img src='https://miro.medium.com/max/500/1*3fA77_mLNiJTSgZFhYnU0Q.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5zP951Bp19Y",
        "colab_type": "text"
      },
      "source": [
        "## Principais aplicações de uma rede neural\n",
        "\n",
        "TO-DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2K_orlZ-vH1",
        "colab_type": "text"
      },
      "source": [
        "# Funções de ativação\n",
        "\n",
        "As funções de ativação tem a responsabilidade de introduzir valores não lineares ao modelo de uma rede neural, habilitando a rede a trabalhar com valores desconhecidos, além das relações lineares entre as variaveis dependentes e independentes.\n",
        "\n",
        "Para um exemplo prático, vejamos o seguinte modelo de uma rede neural de duas camadas(os vieses foram omitidos):\n",
        "\n",
        ">$ y = \\phi(\\phi(\\pmb{X}\\pmb{W_1})\\pmb{W_2})\\pmb{w} $\n",
        "\n",
        "Onde:\n",
        "* $\\pmb{X}$ = Matriz de dados\n",
        "* $\\pmb{W_1}$ = Pesos das camadas oculta\n",
        "* $\\pmb{w}$ = Pesos da camada de saida\n",
        "* $\\phi$ = função de ativação/função não linear\n",
        "\n",
        "Note que caso nós retiremos as funções não lineares do modelo, temos o seguinte resultado:\n",
        "\n",
        ">$ y = \\pmb{X}\\pmb{W_1}\\pmb{W_2}\\pmb{w} $\n",
        "\n",
        "Note que se levarmos em consideração que $ \\pmb{W_1}\\pmb{W_2}\\pmb{w} $ é $\\pmb{u}$, teremos o seguinte resultado:\n",
        "\n",
        ">$ y = \\pmb{X}\\pmb{u} $\n",
        "\n",
        "Este resultado é exatamente um regressão linear, de forma que essa rede neural sem a função de ativação/função não linear, estára sujeita as mesmas restrições dos modelos lineares.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxz7-54zbf2w",
        "colab_type": "text"
      },
      "source": [
        "# Principais tipos de funções de ativação\n",
        "\n",
        "<img src=https://mlfromscratch.com/content/images/2019/12/activation-functions.gif>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOcHlELCJsuP",
        "colab_type": "text"
      },
      "source": [
        "## Função sigmoide\n",
        "---\n",
        "A **função sigmoide** é uma função usada amplamente na matemárica ecomica e computacional, é chamada dessa forma por sua caracteristica grafica em forma de \"S\", chamada de curva de sigmoide. A função sigmoide é utilizada para a criação e transformação de componentes matematicos não lineares.\n",
        "\n",
        "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1280px-Logistic-curve.svg.png width=\"500\">\n",
        "\n",
        "\n",
        "### Definição da função sigmoide\n",
        "\n",
        ">$ \\sigma(x)=\\frac{1}{1+e^x} $\n",
        "\n",
        ">onde: $ e = 2.718 $, constante de euler\n",
        "\n",
        "Sua derivada:\n",
        "\n",
        "> $ \\sigma'(x)=\\sigma(x)(1-\\sigma(x)) $\n",
        "\n",
        "No caso do primeiro algoritmo de exemplo, a função sigmoide está sendo utilizada para converter os valores em probabilidades. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDMRs27bxRqS",
        "colab_type": "text"
      },
      "source": [
        "## E como fica o código de uma função sigmoide?\n",
        "\n",
        "No código utilizamos o método numpy.exp, este método realiza uma exponenciação da constante de Euler pelo parametro passado pelo método\n",
        "\n",
        "* $ euler = 2.718 $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkGNIZpNxWsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7hFEkFIxY3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fe5a8388-86fa-4a8b-efe7-89802f09baea"
      },
      "source": [
        "mmatrix = np.array([[1,2,3],[4,0,6]])\n",
        "print('Valor da matriz [[1,2,3],[4,0,6] ao passar por uma função de ativação ReLu: \\n', sigmoid(mmatrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Valor da matriz [[1,2,3],[4,0,6] ao passar por uma função de ativação ReLu: \n",
            " [[0.73105858 0.88079708 0.95257413]\n",
            " [0.98201379 0.5        0.99752738]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tSmVutuboa3",
        "colab_type": "text"
      },
      "source": [
        "## ReLu \n",
        "---\n",
        "A ativação linear retificada é extremamente semelhante a função identidade, sendo diferente apenas que a ReLu produz zero em metade de seu dominio e como consequencia, as derivadas se mantêm grandes enquanto a unidade estiver ativa.\n",
        "\n",
        "<img src=https://matheusfacure.github.io/img/tutorial/activations/RELU.png width=\"500\">\n",
        "\n",
        "A definição da função ReLu é a seguinte:\n",
        "\n",
        "> $ ReLU(x) = max(0,x) $\n",
        "\n",
        "E sua derivada:\n",
        "\n",
        "> $ ReLU'(x)= \\begin{cases}\n",
        "    \t1, & \\text{se } x\\ge 0\\\\\n",
        "    \t0, & \\text{se } x\\leq 0\n",
        "\t    \\end{cases} $\n",
        "\n",
        "Note que as derivadas não são apenas grandes, mas também estáveis, sendo 1, quando $ x>0 $ e 0 quando $ x<0 $. Note tambem que a segunda derivada é zero em todo o domínio.\n",
        "\n",
        "A ativação ReLu é muito mais eficiente que a ativação sigmoidal por exemplo, contribuindo para popular ainda mais o deep learning e mostrando como algo simples pode ser poderoso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qm10xinbsYE",
        "colab_type": "text"
      },
      "source": [
        "## E como fica o código de uma função ReLu?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X5hpD_k9mHo5",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fq0ltmMmHpH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a94150c1-2169-421c-855f-083b9972509a"
      },
      "source": [
        "mmatrix = np.array([[1,2,3],[4,0,6]])\n",
        "print('Valor da matriz [[1,2,3],[4,0,6] ao passar por uma função de ativação ReLu: \\n', relu(mmatrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Valor da matriz [[1,2,3],[4,0,6] ao passar por uma função de ativação ReLu: \n",
            " [[1 2 3]\n",
            " [4 0 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHyP-opkoe8M",
        "colab_type": "text"
      },
      "source": [
        "## Softmax\n",
        "---\n",
        "A função softmax é utilizada para modelos de predição de multiplas classes principalmente. A função produz saidas entre 0 e 1, sendo que a soma das saidas em modelos de classificação de classes, sera sempre 1.\n",
        "\n",
        "A definição da função Softmax é a seguinte:\n",
        "\n",
        "> $ f(x) = \\frac{\\exp(x_i)}{\\sum \\exp(x_i))} $\n",
        "\n",
        "A principal diferença entre uma função sigmoide e uma função softmax é seu objetivo de uso, enquanto a função sigmoide é custumeiramente utilizada pra classificação binaria a função softmax é utilizada para classificações multiplas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7jakmXgofea",
        "colab_type": "text"
      },
      "source": [
        "## E como fica o código de uma função Softmax?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V63GfvSXofkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    expo = np.exp(X)\n",
        "    expo_sum = np.sum(np.exp(X))\n",
        "    return expo/expo_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjJ2l_pIsUtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "28bc302f-7a69-4e50-a955-edf65c8f6ef8"
      },
      "source": [
        "mmatrix = np.array([[1,2,3],[4,0,6]])\n",
        "print('Valor da matriz [[1,2,3],[4,0,6] ao passar por uma função de ativação \\\n",
        "ReLu: \\n', softmax(mmatrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Valor da matriz [[1,2,3],[4,0,6] ao passar por uma função de ativação ReLu: \n",
            " [[0.00555636 0.01510375 0.04105626]\n",
            " [0.11160249 0.00204407 0.82463706]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkirQkltsilb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "08890b6d-956e-471b-fc84-cf550dbeebef"
      },
      "source": [
        "result = softmax(mmatrix)\n",
        "print('Prova de que a soma dos valores de saida da função sempre será 1: ',\n",
        "      np.sum(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prova de que a soma dos valores de saida da função sempre será 1:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHiFLVBotjoN",
        "colab_type": "text"
      },
      "source": [
        "## Principais casos de uso para cada função\n",
        "\n",
        "* Softmax -> Classificação multipla\n",
        "* Sigmoide -> Classificação binaria\n",
        "* ReLu -> Redes neurais convolucionais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tYcubs5UXAk",
        "colab_type": "text"
      },
      "source": [
        "# Bias/Viés\n",
        "\n",
        "Um Bias/Viés represeta uma variação fixa no calculo de alguma coisa, no caso de redes neurais, utilizamos um valor de bias durante o calculo dos neuronios.\n",
        "\n",
        "Exemplo: Ao se pesar em uma balança convencional, o valor nunca sera 100% preciso, pois, existe o peso dos itens que estão no seu corpo(roupas, relogio e etc), esse valor dos itens é uma variancia fixa e é chamado de Bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phTihzlgxe1B",
        "colab_type": "text"
      },
      "source": [
        "# Funcionamento de uma rede neural\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R5D9wP6Cs6_",
        "colab_type": "text"
      },
      "source": [
        "## Neuronios\n",
        "---\n",
        "Os neuronios são a unidade mais basica de uma rede neural, cada RN pode ter um numero diferente de neuronios com funcionamentos diferentes, mas basicamente, os neuronios recebem um dado, processam ele e devolvem outro dado processado. O processo realizado dentro de cada neuronio é parte crucial para definir o funcionamento das redes neurais.\n",
        "\n",
        "<img src=https://victorzhou.com/a74a19dc0599aae11df7493c718abaf9/perceptron.svg width=\"350\">\n",
        "\n",
        "O exemplo acima representa um neuronio de duas entradas, o processo que está aconcendo dentro dele pode ser definido como o seguinte:\n",
        "\n",
        "Primeiro, cada entrada é multiplicada por um peso:\n",
        "\n",
        "* $ x_1 = x_1 * w_1 $\n",
        "* $ x_2 = x_2 * w_2 $\n",
        "\n",
        "Segundo, o resusltado das entradas multiplicadas pelo peso são somadas a um bias/viés.\n",
        "\n",
        "* $ \\alpha = x_1 + x_2 + b $\n",
        "\n",
        "Terceiro, o resultado passa por uma função de ativação/não linearidade(Explicada a diante):\n",
        "\n",
        "* $ y = f(\\alpha) $\n",
        "\n",
        "No final, a seguinte equação define os neuronios:\n",
        "\n",
        "* $ y = f(x_1 * w_1 + x_2 * w_2 + b) $\n",
        "\n",
        "Para melhor entendimento, segue o exemplo:\n",
        "\n",
        "Parametros:\n",
        "* $ w_1 = 0, w_2 = 1 $\n",
        "* $ b = 4 $\n",
        "* $ f() = \\frac{1}{1+e^x} $ (sigma function)\n",
        "\n",
        "Entradas:\n",
        "* $ x_1 = 2, x_2 = 3 $\n",
        "\n",
        "Ex:\n",
        "\n",
        "* $ y = f(2 * 0 + 3 * 1 + 4) $\n",
        "* $ y = f(7) $\n",
        "* $ y \\approx 0,999 $\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yndJRS2pw32b",
        "colab_type": "text"
      },
      "source": [
        "## Como codar um neuronio?\n",
        "\n",
        "Basicamente, o código de um neuronio compreende todos os processos matematicos que aquele neuronio foi designado pra fazer.\n",
        "\n",
        "Segue exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjLdNg7uxKHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7k5cPBzxLiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neuron:\n",
        "\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    return sigmoid(total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuOkfYxty-fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "adcc47ee-55ab-4ad5-b00a-fb4bc05d2cf9"
      },
      "source": [
        "neuron = Neuron(np.array([0, 2]), 4)\n",
        "print('Valor das entradas 2 e 3 depois do processamento no neuronio: ')\n",
        "print(neuron.feedforward(np.array([2,3])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Valor das entradas 2 e 3 depois do processamento no neuronio: \n",
            "0.9999546021312976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNqqzdXGwuZi",
        "colab_type": "text"
      },
      "source": [
        "## Relacionamento entre neuronios\n",
        "\n",
        "TO-DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EoU7GRswwpx",
        "colab_type": "text"
      },
      "source": [
        "## Camadas de uma rede neural\n",
        "\n",
        "TO-DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q13Pt2YPDpv",
        "colab_type": "text"
      },
      "source": [
        "## Criando uma rede neural de duas camadas do zero\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6rn1LCu7lJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Função sigmoide\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "      return x*(1-x)\n",
        "  return 1/(1+np.exp(-x))\n",
        "    \n",
        "# input dataset\n",
        "X = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1] ])\n",
        "    \n",
        "# output dataset            \n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "# Gera uma seed para os numeros gerados randomicamente, visando a distribuição\n",
        "# homogenea para todas as interações\n",
        "np.random.seed(1)\n",
        "\n",
        "# initialize weights randomly with mean 0\n",
        "syn0 = 2*np.random.random((3,1)) - 1\n",
        "\n",
        "for iter in np.arange(10000):\n",
        "\n",
        "  # forward propagation\n",
        "  l0 = X\n",
        "  l1 = nonlin(np.dot(l0,syn0))\n",
        "\n",
        "  # how much did we miss?\n",
        "  l1_error = y - l1\n",
        "\n",
        "  # multiply how much we missed by the \n",
        "  # slope of the sigmoid at the values in l1\n",
        "  l1_delta = l1_error * nonlin(l1,True)\n",
        "\n",
        "  # update weights\n",
        "  syn0 += np.dot(l0.T,l1_delta)\n",
        "\n",
        "print(\"Output depois do treino:\")\n",
        "print(l1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQAC6Eia4JD1",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparametros\n",
        "\n",
        "Hyperparametros são os atributos que irão definir a arquitetura(Ex: número de neuronios ocultos) do modelo da sua rede neural, bem como como a sua rede neural irá ser treinada(Ex: taxa de treino), esses paraemtros são definidos antes de iniciar o treinamento do modelo e são de extrema importancia para a perfomance, eficacia e acuracia do modelo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ws3gvQCQxp",
        "colab_type": "text"
      },
      "source": [
        "# Principais tipos de Hyperparametros\n",
        "\n",
        "Estes são alguns dos principais hyperparametros que podemos utilziar para trinar alguns modelos de redes neurais, bem como a sua definição e onde podemos utiliza-los."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PyHP3fnCacC",
        "colab_type": "text"
      },
      "source": [
        "### Nº de camadas ocultas\n",
        "---\n",
        "Este parametro define o numero de camadas ocultas entre a camada de entrada e a camada de saida da sua rede neural, é amplamante utilizada em todas as redes neurais.\n",
        "\n",
        "Basicamente, estas camadas ocultas são utilizadas para melhorar a acuracia do seu modelo, pode-se adicionar novas camadas até que não faça mais diferença se adicionar mais alguma.\n",
        "\n",
        "Varias camadas com boas tecnicas de regularização podem aumentar muito a acuracia do modelo, já se não forem utilizadas ou utilizar poucas, pode acabar gerando um efeito de *underfitting*.\n",
        "\n",
        "*“Very simple. Just keep adding layers until the test error does not improve anymore.”*\n",
        "\n",
        "* Utilizado em: Arquitetura do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuCSiC7bENDo",
        "colab_type": "text"
      },
      "source": [
        "## Dropout\n",
        "---\n",
        "Dropout é um parametro utilizado para evitar *overfitting*, aumentar a acuracia e melhorar o poder de generalização.\n",
        "\n",
        "Geralmente, se utiliza o dropout com valores entre 10% a 50%(porcentagens fazenbdo sempre referencia ao numero de neuronios), 20% pode ser um ponto pra começar a testar a eficacia do parametro. Caso o valor for muito baixo para o tamanho do modelo, o efeito será quase imperceptivel, caso for muito grande, o modelo irá perder poder de aprendizagem.\n",
        "\n",
        "Normalmente o dropout tem mais efeito em redes neurais grandes, dando mais oportunidade da rede neural estabelecer novas representações e conexões.\n",
        "\n",
        "* Utilizado em: Arquitetura do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw-sOQ1zLadE",
        "colab_type": "text"
      },
      "source": [
        "## Função de ativação\n",
        "---\n",
        "Como ja explicado anteriormente, as funções de ativação são utilizadas para introduzir elementos de não linearidade ao modelo de rede neural. \n",
        "\n",
        "Para mais informações, consultar \"Função de ativação de uma rede neural\"\n",
        "\n",
        "* Utilizado em: Arquitetura do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVZn_lPCM_Iq",
        "colab_type": "text"
      },
      "source": [
        "## Taxa de aprendizado\n",
        "---\n",
        "A taxa de aprendizado define o quão rapido o modelo irá atualizar seus parametros durante o treinamento, uma baixa taxa de aprendizado pode atrasar o treinamento, contudo, em algum momento irá normalizar. Altas taxas de treinamento pode acelerar o treinamento do modelo, contudo, pode não haver normalização e o modelo não convergir para sua linha natural.\n",
        "\n",
        "Normalmente, se prefere uma taxa de aprendizado decadente.\n",
        "\n",
        "* Utilizado em: Processo de treinamento do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAZsaM7nNs9l",
        "colab_type": "text"
      },
      "source": [
        "## Momentum\n",
        "---\n",
        "\n",
        "Este parametro é utilizado para direcionar o aprendizado do modelo de acordo com o resultado da geração de treinamento anterior, ajudando a reduzir oscilações.\n",
        "\n",
        "Normalmente, é preferivel manter este parametro entre 0,5 e 0,9.\n",
        "\n",
        "* Utilizado em: Processo de treinamento do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IJ9d9mUT3hz",
        "colab_type": "text"
      },
      "source": [
        "## Numero de *Epochs*\n",
        "---\n",
        "\n",
        "O numero de *epochs* define o numero de vezes que o dataset de treino irá passar pelo modelo e consequentenmente, quantas vezes o modelo irá treinar utilizando esses dados.\n",
        "\n",
        "Caso o modelo não treine por epocas suficientes, ele não ficará preciso, caso treine por epocas demais, pode ocorrer o efeito de *overfitting*. \n",
        "\n",
        "* Utilizado em: Processo de treinamento do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5GsMj76Uce8",
        "colab_type": "text"
      },
      "source": [
        "## Batch size\n",
        "---\n",
        "\n",
        "O tamanho do *batch* define o tamanho dos subgrupos de dados que serão \"entregues\" ao modelo depois de cada atualização de parametros.\n",
        "\n",
        "Normalmente, o tamanho padrão é definido por multiplos de 32 sendo o tamanho 128 o mais utilizado.\n",
        "\n",
        "* Utilizado em: Processo de treinamento do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rNpeGUN0Duq",
        "colab_type": "text"
      },
      "source": [
        "# Modelo de rede neural generica\n",
        "\n",
        "TO-DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i06Uxb7MBnus",
        "colab_type": "text"
      },
      "source": [
        "# Exemplos e implementações"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNsuohZB77tb",
        "colab_type": "text"
      },
      "source": [
        "# Troubleshooting\n",
        "\n",
        "Resolução de alguns dos problemas mais comuns que podem acontecer ao definir e treinar seu modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc1HvFS479x3",
        "colab_type": "text"
      },
      "source": [
        "## Input 0 is incompatible with layer lstm_xx: expected ndim=3, found ndim=2 in Keras\n",
        "\n",
        "Este problema acontece pois as cadamadas do seu modelo não estão compartilhando o mesmo input/output.\n",
        "\n",
        "Por exemplo, caso você tenha duas camadas LSTM seguidas, onde apenas a primeira tem definida o formato do input e o parametro *return_sequences* não estiver presente, é muito provavel que você tera esse erro, para resolver, apenas passe o parametro *return_sequences* para True, isto fara com que o output da primeira camada esteja de acordo com o input da segunda camada. "
      ]
    }
  ]
}