{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --no-cache-dir tensorflow-rocm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: tensorflow in /home/vicotrbb/.local/lib/python3.8/site-packages (2.3.1)\nRequirement already satisfied: tensorflow-estimator&lt;2.4.0,&gt;=2.3.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: absl-py&gt;=0.7.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (0.10.0)\nRequirement already satisfied: google-pasta&gt;=0.1.8 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: keras-preprocessing&lt;1.2,&gt;=1.1.1 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: gast==0.3.3 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (0.3.3)\nRequirement already satisfied: numpy&lt;1.19.0,&gt;=1.16.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (1.18.5)\nRequirement already satisfied: grpcio&gt;=1.8.6 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (1.32.0)\nRequirement already satisfied: wheel&gt;=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\nRequirement already satisfied: wrapt&gt;=1.11.1 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: six&gt;=1.12.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: h5py&lt;2.11.0,&gt;=2.10.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: tensorboard&lt;3,&gt;=2.3.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: astunparse==1.6.3 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: protobuf&gt;=3.9.2 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: setuptools&gt;=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (45.2.0)\nRequirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (0.4.1)\nRequirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (1.22.1)\nRequirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (1.7.0)\nRequirement already satisfied: markdown&gt;=2.6.8 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (3.3)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (2.22.0)\nRequirement already satisfied: werkzeug&gt;=0.11.15 in /home/vicotrbb/.local/lib/python3.8/site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (1.0.1)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (1.3.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &quot;3.5&quot; in /home/vicotrbb/.local/lib/python3.8/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (4.6)\nRequirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /home/vicotrbb/.local/lib/python3.8/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (4.1.1)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/lib/python3/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (0.2.1)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (3.1.0)\nRequirement already satisfied: pyasn1&gt;=0.1.3 in /usr/lib/python3/dist-packages (from rsa&lt;5,&gt;=3.1.4; python_version &gt;= &quot;3.5&quot;-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (0.4.2)\n"
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/vicotrbb/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/vicotrbb/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ''\n",
    "maxlen = 60\n",
    "step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.4.0\n2.3.1\n1.18.5\n3.5\n"
    }
   ],
   "source": [
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "print(np.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed's\n",
    "os.environ['PYTHONHASHSEED']=str(66)\n",
    "tf.random.set_seed(66)\n",
    "np.random.seed(66)\n",
    "random.seed(66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Mapping, Container\n",
    "from sys import getsizeof\n",
    " \n",
    "def deep_getsizeof(o, ids):\n",
    "    d = deep_getsizeof\n",
    "    if id(o) in ids:\n",
    "        return 0\n",
    " \n",
    "    r = getsizeof(o)\n",
    "    ids.add(id(o))\n",
    " \n",
    "    if isinstance(o, str):\n",
    "        return r / 1048576\n",
    " \n",
    "    if isinstance(o, Mapping):\n",
    "        return (r + sum(d(k, ids) + d(v, ids) for k, v in o.iteritems())) / 1048576\n",
    " \n",
    "    if isinstance(o, Container):\n",
    "        return (r + sum(d(x, ids) for x in o)) / 1048576\n",
    "    \n",
    "    return r / 1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(dataFile, parts):\n",
    "    f = open(dataFile,)\n",
    "    data = json.load(f)\n",
    "\n",
    "    content = list(data[x] for x in data.keys())\n",
    "    text = ''\n",
    "\n",
    "    for c in content[:parts]:\n",
    "        for i in c:\n",
    "            text += i\n",
    "\n",
    "    print(f'Corpus length: {len(text)}')\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    text = ''\n",
    "    for c in words:\n",
    "        text += c\n",
    "        text += ' '\n",
    "    text = text.strip()\n",
    "\n",
    "    print(f'Finished to load file')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Corpus length: 1359108\nFinished to load file\n"
    }
   ],
   "source": [
    "text = prepareData('projects/wikipedia-nlp/wikipedia-content-dataset.json', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Memory size: 1.80 MB\n"
    }
   ],
   "source": [
    "print(\"Memory size: %.2f MB\" % deep_getsizeof(text, set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Numero de sequencias: 314412\n"
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Numero de sequencias:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Caracteres unicos: 277\n"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Caracteres unicos:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Vetorizando o texto\nFinalizado vetorização do texto\nX shape:  (314412, 60, 277)\nY shape:  (314412, 277)\n"
    }
   ],
   "source": [
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "print('Vetorizando o texto')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print('Finalizado vetorização do texto')\n",
    "\n",
    "print(\"X shape: \", x.shape)\n",
    "print(\"Y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Memory size of x: 4983.45 MB\nMemory size of y: 83.06 MB\n\n\nMemory size of x: 4.87 GB\nMemory size of y: 0.08 GB\n"
    }
   ],
   "source": [
    "print(\"Memory size of x: %.2f MB\" % deep_getsizeof(x, set()))\n",
    "print(\"Memory size of y: %.2f MB\" % deep_getsizeof(y, set()))\n",
    "print('\\n')\n",
    "print(\"Memory size of x: %.2f GB\" % (deep_getsizeof(x, set())  / 1024))\n",
    "print(\"Memory size of y: %.2f GB\" % (deep_getsizeof(y, set()) / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(len(chars), return_sequences=True, input_shape=(maxlen, len(chars))),\n",
    "    LSTM(len(chars), return_sequences=True),\n",
    "    LSTM(len(chars)),\n",
    "    Dense(len(chars), activation='relu'),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Treino do modelo\")\n",
    "model.fit(x, y, batch_size=128, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}