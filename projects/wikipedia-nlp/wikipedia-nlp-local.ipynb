{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --no-cache-dir tensorflow-rocm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ''\n",
    "maxlen = 60\n",
    "step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed's\n",
    "os.environ['PYTHONHASHSEED']=str(66)\n",
    "tf.random.set_seed(66)\n",
    "np.random.seed(66)\n",
    "random.seed(66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(dataFile, parts):\n",
    "    f = open(dataFile,)\n",
    "    data = json.load(f)\n",
    "\n",
    "    content = list(data[x] for x in data.keys())\n",
    "    text = ''\n",
    "\n",
    "    for c in content[:parts]:\n",
    "        for i in c:\n",
    "            text += i\n",
    "\n",
    "    print(f'Corpus length: {len(text)}')\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    text = ''\n",
    "    for c in words:\n",
    "        text += c\n",
    "        text += ' '\n",
    "    text = text.strip()\n",
    "\n",
    "    print(f'Finished to load file')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = prepareData('Carreira/PROJETOS/PESSOAIS/machine_learning/projects/wikipedia-nlp/wikipedia-content-dataset.json', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Numero de sequencias:', len(sentences))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Caracteres unicos:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "print('Vetorizando o texto')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print('Finalizado vetorização do texto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(len(chars), return_sequences=True, input_shape=(maxlen, len(chars))),\n",
    "    LSTM(len(chars), return_sequences=True),\n",
    "    LSTM(len(chars)),\n",
    "    Dense(len(chars), activation='relu'),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Treino do modelo\")\n",
    "model.fit(x, y, batch_size=128, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}